{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('nlp').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e83ffd00-c04a-44f8-a93e-837e03e94341"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#tokenization in pyspark\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"091e5d14-2c22-4e74-85ce-3d98e19181d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sen_df = spark.createDataFrame([\n    (0,'Hi I heard about spark'),\n    (1, 'I wish java could use case classes'),\n    (2, 'logistic,regression,models,are,neat')\n    \n], ['id','sentence'])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ad00d1-87e8-4b52-8a49-148f298af1bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sen_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6b69522-db07-43b2-ab25-c508056405d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+\n| id|            sentence|\n+---+--------------------+\n|  0|Hi I heard about ...|\n|  1|I wish java could...|\n|  2|logistic,regressi...|\n+---+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+\n| id|            sentence|\n+---+--------------------+\n|  0|Hi I heard about ...|\n|  1|I wish java could...|\n|  2|logistic,regressi...|\n+---+--------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\nregextokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern=\"\\\\W\")\ncount_tokens = udf(lambda words: len(words), IntegerType())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37fa4ea4-d9e1-4c57-a806-51b23ad6d090"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["tokenized = tokenizer.transform(sen_df)\ntokenized.withColumn('tokens', count_tokens(col('words'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ba0b3e6-cd13-4215-a55d-796626312155"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish java could...|[i, wish, java, c...|     7|\n|  2|logistic,regressi...|[logistic,regress...|     1|\n+---+--------------------+--------------------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish java could...|[i, wish, java, c...|     7|\n|  2|logistic,regressi...|[logistic,regress...|     1|\n+---+--------------------+--------------------+------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rg_tokenizer = regextokenizer.transform(sen_df)\nrg_tokenizer.withColumn('tokens', count_tokens(col('words'))).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8bf3c83-7203-4f54-9afe-1bf81daabbac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish java could...|[i, wish, java, c...|     7|\n|  2|logistic,regressi...|[logistic, regres...|     5|\n+---+--------------------+--------------------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish java could...|[i, wish, java, c...|     7|\n|  2|logistic,regressi...|[logistic, regres...|     5|\n+---+--------------------+--------------------+------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nsen_df = spark.createDataFrame([\n    (0,'Hi I heard about spark'),\n    (1, 'I wish java could use case classes'),\n    (2, 'logistic regression models are neat')\n    \n], ['id','sentence'])\nremover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\nremover.transform(sen_df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stopword removal","showTitle":true,"inputWidgets":{},"nuid":"9d73f338-dde8-4976-aabe-d9fdccc7db66"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-173559009452800>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m ], ['id','sentence'])\n\u001B[1;32m      8\u001B[0m \u001B[0mremover\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStopWordsRemover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tokens'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'filtered'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mremover\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msen_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    215\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Params must be a param map but got %s.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    348\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 350\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    351\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: tokens does not exist. Available: id, sentence","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: tokens does not exist. Available: id, sentence","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-173559009452800>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m ], ['id','sentence'])\n\u001B[1;32m      8\u001B[0m \u001B[0mremover\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStopWordsRemover\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tokens'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'filtered'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mremover\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msen_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    215\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Params must be a param map but got %s.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    348\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    349\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 350\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    351\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: tokens does not exist. Available: id, sentence"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\nwordDataFrame = spark.createDataFrame([\n    (0, ['hi','i','heard','about','sparl']),\n    (1,['i','wish','java','could','use','case','classes']),\n    (2,['logistics','regression','models','are','neat'])\n],['id','words'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"N grams","showTitle":true,"inputWidgets":{},"nuid":"f7b08e2f-744f-42e9-9012-91ed4d9606ff"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ngram = NGram(n=2, inputCol='words', outputCol='grams')\nngram.transform(wordDataFrame).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30ae0015-716b-450e-a474-842a744e4748"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------+--------------------+\n| id|               words|               grams|\n+---+--------------------+--------------------+\n|  0|[hi, i, heard, ab...|[hi i, i heard, h...|\n|  1|[i, wish, java, c...|[i wish, wish jav...|\n|  2|[logistics, regre...|[logistics regres...|\n+---+--------------------+--------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------+--------------------+\n| id|               words|               grams|\n+---+--------------------+--------------------+\n|  0|[hi, i, heard, ab...|[hi i, i heard, h...|\n|  1|[i, wish, java, c...|[i wish, wish jav...|\n|  2|[logistics, regre...|[logistics regres...|\n+---+--------------------+--------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["ngram.transform(wordDataFrame).select('grams').show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"585709c0-c12f-4eb6-9cbd-9a9a95381d87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------------------------------------------------------+\n|grams                                                             |\n+------------------------------------------------------------------+\n|[hi i, i heard, heard about, about sparl]                         |\n|[i wish, wish java, java could, could use, use case, case classes]|\n|[logistics regression, regression models, models are, are neat]   |\n+------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------------------------------------------------------+\n|grams                                                             |\n+------------------------------------------------------------------+\n|[hi i, i heard, heard about, about sparl]                         |\n|[i wish, wish java, java could, could use, use case, case classes]|\n|[logistics regression, regression models, models are, are neat]   |\n+------------------------------------------------------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\nsen_df = spark.createDataFrame([\n    (0,'Hi I heard about spark'),\n    (1, 'I wish java could use case classes'),\n    (2, 'logistic regression models are neat')],['id','sentence'])\n\ntokenizer = Tokenizer(inputCol='sentence', outputCol='words')\nword_data = tokenizer.transform(sen_df)\n\nword_data.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"TFIDF","showTitle":true,"inputWidgets":{},"nuid":"7ed654e7-29d3-4cbd-a685-c7d5087c1434"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-----------------------------------+------------------------------------------+\n|id |sentence                           |words                                     |\n+---+-----------------------------------+------------------------------------------+\n|0  |Hi I heard about spark             |[hi, i, heard, about, spark]              |\n|1  |I wish java could use case classes |[i, wish, java, could, use, case, classes]|\n|2  |logistic regression models are neat|[logistic, regression, models, are, neat] |\n+---+-----------------------------------+------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----------------------------------+------------------------------------------+\n|id |sentence                           |words                                     |\n+---+-----------------------------------+------------------------------------------+\n|0  |Hi I heard about spark             |[hi, i, heard, about, spark]              |\n|1  |I wish java could use case classes |[i, wish, java, could, use, case, classes]|\n|2  |logistic regression models are neat|[logistic, regression, models, are, neat] |\n+---+-----------------------------------+------------------------------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#term frequency\nhashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')\nfeaturized_data = hashing_tf.transform(word_data)\n#idf\nidf = IDF(inputCol='rawFeatures', outputCol='features')\nidf_model = idf.fit(featurized_data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"483653a1-7a4c-43c1-97e2-67364b6d16be"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rescaled_data = idf_model.transform(featurized_data)\nrescaled_data.select('id','features').show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6f4a027-3806-4795-a70e-85c6ace2f16d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|id |features                                                                                                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|0  |(262144,[18700,19036,33808,66273,173558],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n|1  |(262144,[19036,20719,55551,58672,98717,109547,192310],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n|2  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|id |features                                                                                                                                                                                      |\n+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|0  |(262144,[18700,19036,33808,66273,173558],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n|1  |(262144,[19036,20719,55551,58672,98717,109547,192310],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n|2  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\ndf = spark.createDataFrame([\n    (0, 'a b c'.split(\" \")),\n    (1,'a b b c a'.split(' '))], \n['id', 'words'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Count vectorizer","showTitle":true,"inputWidgets":{},"nuid":"b70b58df-2c81-4e90-8465-da96c62c627f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------------+\n| id|          words|\n+---+---------------+\n|  0|      [a, b, c]|\n|  1|[a, b, b, c, a]|\n+---+---------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------------+\n| id|          words|\n+---+---------------+\n|  0|      [a, b, c]|\n|  1|[a, b, b, c, a]|\n+---+---------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2.0)\nmodel = cv.fit(df)\nresults = model.transform(df)\nresults.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10171152-8e4c-40a9-bece-2eb5ce5d0b68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b199999a-0dae-4a85-a03c-90917a97765c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Basic NLP tools in pyspark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":173559009452790}},"nbformat":4,"nbformat_minor":0}
